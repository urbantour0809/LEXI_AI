import os
import chromadb
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import re

#  ChromaDB ì„¤ì •
CHROMA_DB_PATH = "../dataset/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
legal_cases_collection = chroma_client.get_or_create_collection(name="legal_cases")  # ë²•ë¥  ë°ì´í„°
legal_precedents_collection = chroma_client.get_or_create_collection(name="legal_precedents")  # íŒë¡€ ë°ì´í„°

#  Fine-tuned `legal-bert-base` ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../ft_legal_bert/checkpoint-1185"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
embedding_model = AutoModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16).to(device)

def embed_text(text):
    """  Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë²¡í„°í™” (GPU í™œìš©) """

    if text is None or not isinstance(text, str) or not text.strip():
        raise ValueError("ì˜¤ë¥˜: `text` ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    text = re.sub(r"\s+", " ", text.strip())  # ì—¬ëŸ¬ ê°œ ê³µë°± ì œê±°

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to(device)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)

    return embedding.cpu().numpy().flatten().tolist()

def get_relevant_docs(query, top_k=5):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ê³ , ê´€ë ¨ ë²•ë¥  ë° íŒë¡€ ë°ì´í„°ë¥¼ ê²€ìƒ‰ """

    if not isinstance(query, str) or not query.strip():
        raise ValueError("ì˜¤ë¥˜: ì…ë ¥ëœ ì§ˆë¬¸ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    query_embedding = embed_text(query)

    results_cases = legal_cases_collection.query(query_embeddings=[query_embedding], n_results=top_k)
    results_precedents = legal_precedents_collection.query(query_embeddings=[query_embedding], n_results=top_k)

    relevant_texts, sources, scores, law_numbers = [], [], [], []

    def process_results(results):
        # ê²€ìƒ‰ëœ ê²°ê³¼ ê°€ê³µ
        for i, meta_list in enumerate(results.get("metadatas", [])):
            for meta in meta_list:
                text = meta.get("text", results["documents"][i] if results.get("documents") else None)
                score = results.get("distances", [])[i] if results.get("distances") else 0.0

                # "ì‚¬ê±´ë²ˆí˜¸"ë¥¼ ì˜¬ë°”ë¥´ê²Œ ê°€ì ¸ì˜¤ê¸° (ë²•ë¥  & íŒë¡€ êµ¬ë¶„)
                case_no = meta.get("case_no") or meta.get("ì‚¬ê±´ë²ˆí˜¸") or meta.get("law_number") or "ì‚¬ê±´ë²ˆí˜¸ ì—†ìŒ"

                # ì‚¬ê±´ë²ˆí˜¸ê°€ `text` ë‚´ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê°€ì ¸ì˜¤ê¸° (ì˜ˆì™¸ ì²˜ë¦¬)
                if case_no == "ì‚¬ê±´ë²ˆí˜¸ ì—†ìŒ" and text:
                    match = re.search(r"ğŸ“Œ ì‚¬ê±´ë²ˆí˜¸: ([\w\d-]+)", text)
                    if match:
                        case_no = match.group(1)

                # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                if isinstance(text, list):
                    text = "\n".join([t if t is not None else "" for t in text])

                if text and text.strip():
                    relevant_texts.append(text)
                    sources.append(case_no)  # ì‚¬ê±´ë²ˆí˜¸ë¥¼ ì¶œì²˜ ì •ë³´ë¡œ ì‚¬ìš©
                    scores.append(score)
                    law_numbers.append(case_no)  # ì‚¬ê±´ë²ˆí˜¸ë¥¼ ë²•ë¥  ë²ˆí˜¸ë¡œ ì €ì¥



    process_results(results_cases)
    process_results(results_precedents)

    # 2ì°¨ì› ë¦¬ìŠ¤íŠ¸(`list[list[float]]`)ì¸ ê²½ìš° 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    scores = [s for sublist in scores for s in (sublist if isinstance(sublist, list) else [sublist])]

    return relevant_texts, sources, scores, law_numbers

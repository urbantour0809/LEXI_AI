import os
import json
import chromadb
import torch
from transformers import AutoModel, AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
BASE_DIR = "../ë²•ë¥ , ê·œì •/01.ë°ì´í„°"
TRAINING_PATH = os.path.join(BASE_DIR, "1.Training", "ë¼ë²¨ë§ë°ì´í„°", "TL_1.íŒê²°ë¬¸")
VALIDATION_PATH = os.path.join(BASE_DIR, "2.Validation", "ë¼ë²¨ë§ë°ì´í„°", "VL_1.íŒê²°ë¬¸")

# GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# ChromaDB ì„¤ì •
CHROMA_DB_PATH = "../dataset/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
legal_cases_collection = chroma_client.get_or_create_collection(name="legal_cases")

# Fine-tuned `legal-bert-base` ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../ft_legal_bert/checkpoint-1185"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
embedding_model = AutoModel.from_pretrained(MODEL_PATH).to(device)

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (TF-IDF í™œìš©)
STOPWORDS = set([
    "ìˆë‹¤", "í•˜ëŠ”", "ë˜ì–´", "ë•Œë¬¸", "ìœ„í•´", "ê·¸ë¦¬ê³ ", "ë”°ë¼ì„œ", "í•˜ì§€ë§Œ", "ëŒ€í•˜ì—¬",
    "ê°™ì€", "ê·¸ëŸ¬í•œ", "ê²½ìš°", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "ì´ë‹¤", "ì˜€ë‹¤", "ë˜ì§€", "ì—†ì´",
    "í•˜ëŠ”", "í•˜ë©°", "ë¼ê³ ", "ê¹Œì§€", "í•˜ë©´ì„œ", "ë“±ë“±", "ë°", "ì´ë‹¤"
])

def extract_keywords(text, top_n=5):
    """ TF-IDFë¥¼ í™œìš©í•˜ì—¬ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ """
    if not text.strip():
        return "í‚¤ì›Œë“œ ì—†ìŒ"
    
    vectorizer = TfidfVectorizer(stop_words=list(STOPWORDS), max_features=top_n)
    try:
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_names = vectorizer.get_feature_names_out()
        return ", ".join(feature_names) if feature_names.size > 0 else "í‚¤ì›Œë“œ ì—†ìŒ"
    except ValueError:
        return "í‚¤ì›Œë“œ ì—†ìŒ"

def embed_text(text):
    """ Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë²¡í„°í™” (GPU í™œìš©) """
    if not isinstance(text, str) or not text.strip():
        return None

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to(device)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy().tolist()[0]

def extract_text_from_json(json_file):
    """ JSON íŒŒì¼ì—ì„œ ë²•ë¥  ë°ì´í„° ì¶”ì¶œ """
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        case_no = data["info"].get("caseNo", "").strip()
        case_name = data["info"].get("caseNm", "").strip()
        related_laws = "\n".join(data["info"].get("relateLaword", ["ê´€ë ¨ ë²•ë¥  ì—†ìŒ"]))
        disposal = "\n".join(data["disposal"].get("disposalcontent", ["ì²˜ë¶„ ë‚´ìš© ì—†ìŒ"]))
        mentioned_items = "\n".join(data["mentionedItems"].get("rqestObjet", ["ê´€ë ¨ ì²­êµ¬ ì‚¬í•­ ì—†ìŒ"]))
        assrs = "\n".join(data["assrs"].get("acusrAssrs", ["ì£¼ìš” ì£¼ì¥ ì—†ìŒ"]))
        facts = "\n".join(data["facts"].get("bsisFacts", ["ì‚¬ì‹¤ ê´€ê³„ ì—†ìŒ"]))
        dcss = "\n".join(data["dcss"].get("courtDcss", ["ë²•ì›ì˜ íŒë‹¨ ì—†ìŒ"]))
        closing = "\n".join(data["close"].get("cnclsns", ["ê²°ë¡  ì—†ìŒ"]))

        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        if not case_no or not case_name:
            return None, None, None, None

        text_data = f"""
        ğŸ“Œ ì‚¬ê±´ë²ˆí˜¸: {case_no}
        ğŸ“Œ ì‚¬ê±´ëª…: {case_name}
        ğŸ“Œ ê´€ë ¨ ë²•ë¥ : {related_laws}
        ğŸ“Œ ì²˜ë¶„ ë‚´ìš©: {disposal}
        ğŸ“Œ ê´€ë ¨ ì²­êµ¬ ì‚¬í•­: {mentioned_items}
        ğŸ“Œ ì£¼ìš” ì£¼ì¥: {assrs}
        ğŸ“Œ ì‚¬ì‹¤ ê´€ê³„: {facts}
        ğŸ“Œ ë²•ì›ì˜ íŒë‹¨: {dcss}
        ğŸ“Œ ê²°ë¡ : {closing}
        """

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆ ê²½ìš° "í‚¤ì›Œë“œ ì—†ìŒ")
        keywords = extract_keywords(f"{related_laws} {disposal} {facts} {dcss}")

        return text_data.strip(), case_no, case_name, keywords
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {json_file} - {e}")
        return None, None, None, None

def process_files(base_path, collection):
    """ ì—°ë„ë³„ í´ë”ê¹Œì§€ ìˆœíšŒí•˜ì—¬ JSON íŒŒì¼ì„ ë²¡í„°í™” """
    for year_folder in ["1981~2016", "2017", "2018", "2019", "2020", "2021"]:
        case_path = os.path.join(base_path, "01.ë¯¼ì‚¬", year_folder)

        if not os.path.exists(case_path):
            print(f"âš  í´ë” ì—†ìŒ: {case_path}")
            continue

        for file_name in os.listdir(case_path):
            if not file_name.endswith(".json"):
                continue

            file_path = os.path.join(case_path, file_name)
            text_data, case_no, case_name, keywords = extract_text_from_json(file_path)

            if not text_data or not case_no or not case_name:
                print(f"âš  ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€: {file_name}")
                continue

            embedding = embed_text(text_data)
            if embedding is None:
                print(f"âš  ì„ë² ë”© ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€: ì‚¬ê±´ë²ˆí˜¸ {case_no}")
                continue

            # ì¤‘ë³µ í™•ì¸ í›„ ë°ì´í„° ì¶”ê°€
            existing_data = collection.get(ids=[case_no])
            if existing_data and existing_data["ids"]:
                print(f"ê¸°ì¡´ ë°ì´í„° ìŠ¤í‚µ: ì‚¬ê±´ë²ˆí˜¸ {case_no}")
                continue

            # ë°ì´í„° ì¶”ê°€
            print(f"ë²•ë¥  ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: ì‚¬ê±´ë²ˆí˜¸ {case_no}")
            collection.add(
                ids=[case_no],
                embeddings=[embedding],
                metadatas=[{
                    "case_number": case_no,
                    "title": case_name,
                    "keywords": keywords,
                    "text": text_data
                }]
            )

# ì‹¤í–‰
if __name__ == "__main__":
    print("Training ë°ì´í„° ë²¡í„°í™” ì‹œì‘...")
    process_files(TRAINING_PATH, legal_cases_collection)

    print("Validation ë°ì´í„° ë²¡í„°í™” ì‹œì‘...")
    process_files(VALIDATION_PATH, legal_cases_collection)

    print("âœ… ëª¨ë“  ë²•ë¥  ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ!")

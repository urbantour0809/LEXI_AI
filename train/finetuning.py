import os
import json
import torch
import transformers
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

# GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_NAME = "nlpaueb/legal-bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ë²•ë¥  ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì—°ë„ë³„ í´ë” í¬í•¨)
def load_legal_data(json_folder):
    """ ì—°ë„ë³„ í´ë”ê¹Œì§€ ìˆœíšŒí•˜ë©° ë¯¼ì‚¬ íŒê²°ë¬¸ ë°ì´í„° ë¡œë“œ """
    data = []

    for year_folder in os.listdir(json_folder):
        year_path = os.path.join(json_folder, year_folder)
        if not os.path.isdir(year_path):
            continue  # í´ë”ê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ

        for file_name in os.listdir(year_path):
            if file_name.endswith(".json"):
                file_path = os.path.join(year_path, file_name)

                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        case_data = json.load(f)

                    # JSON í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
                    case_no = case_data.get("info", {}).get("caseNo", "ì‚¬ê±´ë²ˆí˜¸ ì—†ìŒ")
                    case_name = case_data.get("info", {}).get("caseNm", "ì‚¬ê±´ëª… ì—†ìŒ")
                    related_laws = "\n".join(case_data.get("info", {}).get("relateLaword", ["ê´€ë ¨ ë²•ë¥  ì—†ìŒ"]))
                    facts = "\n".join(case_data.get("facts", {}).get("bsisFacts", ["ì‚¬ì‹¤ ê´€ê³„ ì—†ìŒ"]))

                    # í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ë¬¸ì¥ êµ¬ì„±
                    text = f"ì‚¬ê±´ëª…: {case_name}\nì‚¬ê±´ë²ˆí˜¸: {case_no}\nê´€ë ¨ ë²•ë¥ : {related_laws}\nì‚¬ì‹¤ ê´€ê³„: {facts}"
                    label = 1  # ë‹¨ìˆœ ì´ì§„ ë¶„ë¥˜

                    data.append((text, label))

                except json.JSONDecodeError:
                    print(f"âŒ JSON íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {file_name}")
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_name} - {e}")

    return data

# ë°ì´í„° ê²½ë¡œ ì„¤ì • (í•„í„°ë§ëœ ë²•ë¥  ë°ì´í„° í´ë”)
TRAINING_PATH = "../ë²•ë¥ , ê·œì •/01.ë°ì´í„°/1.Training/ë¼ë²¨ë§ë°ì´í„°/TL_1.íŒê²°ë¬¸/01.ë¯¼ì‚¬"
VALIDATION_PATH = "../ë²•ë¥ , ê·œì •/01.ë°ì´í„°/2.Validation/ë¼ë²¨ë§ë°ì´í„°/VL_1.íŒê²°ë¬¸/01.ë¯¼ì‚¬"

# ë°ì´í„° ë¡œë“œ
train_data = load_legal_data(TRAINING_PATH)
val_data = load_legal_data(VALIDATION_PATH)

print(f"ğŸ“Œ Training ë°ì´í„° ê°œìˆ˜: {len(train_data)}")
print(f"ğŸ“Œ Validation ë°ì´í„° ê°œìˆ˜: {len(val_data)}")

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class LegalDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, label = self.data[idx]
        encoding = self.tokenizer(text, truncation=True, padding="max_length", max_length=self.max_length, return_tensors="pt")

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# ë°ì´í„°ì…‹ ì¤€ë¹„
train_dataset = LegalDataset(train_data, tokenizer)
val_dataset = LegalDataset(val_data, tokenizer)

# ëª¨ë¸ ë¡œë“œ (ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ í—¤ë“œ ì¶”ê°€)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)

# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • (Colab í™˜ê²½ ìµœì í™”)
training_args = TrainingArguments(
    output_dir="./legal_bert_finetuned",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    per_device_train_batch_size=8,  # Colab GPU ìµœì í™”
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,  # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    num_train_epochs=5,
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    report_to="none"  # WandB ë¡œê·¸ ë¹„í™œì„±í™”
)

# Trainer ì„¤ì • (GPU ì‚¬ìš©)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

# í•™ìŠµ ì‹œì‘
if __name__ == "__main__":
    print("ğŸ“Œ `legal-bert-base` ëª¨ë¸ Fine-tuning ì‹œì‘...")

    # ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(train_dataset) == 0 or len(val_dataset) == 0:
        raise ValueError("âŒ Training ë˜ëŠ” Validation ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    trainer.train()

    # ëª¨ë¸ ì €ì¥
    model.save_pretrained("./legal_bert_finetuned")
    tokenizer.save_pretrained("./legal_bert_finetuned")

    print("Fine-tuning ì™„ë£Œ! ëª¨ë¸ì´ `./legal_bert_finetuned` í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
